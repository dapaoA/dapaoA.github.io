---
title: 论文阅读记录2
categories: [分布式ML]
abbrlink: 21001
date: 2021-1-17 01:30:01
---

# 函数条件总结

重新开始看论文，感觉东西太过繁杂，我长期记忆又不行，边看边写写总结：目前还在瞎看阶段，之后有时间再总结。

SC,ESC,WSC,RSI,EB,PL,QG



### ESC

$f(x)-f(y) \geq\langle\nabla f(y), x-y\rangle+\frac{l}{2}\|x-y\|^{2} \quad$ for all $x, y \in \Omega$ with $\mathcal{P}_{S}(x)=\mathcal{P}_{S}(y)$ 

[Liu et al., 2014]里的，They describe an asynchronous parallel stochastic coordinate descent algorithm for minimizing smooth unconstrained or separably constrained functions. The method achieves a linear convergence rate on functions that satisfy an essential strong convexity property and a sublinear rate (1/K) on general convex functions.

ESC不要求所有的解都满足SC，只要求解在非空的最优解上的投影一致的集合里满足。给出来的例子有：

$-f(A x)$ with arbitrary linear transformation $A,$ where $f(\cdot)$ is strongly convex; $-f(x)=\max \left(a^{T} x-b, 0\right)^{2},$ for $a \neq 0$

### WSC

$f^{*} \geq f(x)+\langle\nabla f(x), \bar{x}-x\rangle+\frac{\kappa_{f}}{2}\|x-\bar{x}\|^{2} \quad \forall x \in X$

[Necoara et al., 2015], 定义为quasi-strongly convex, 且 $\bar{x}=[x]_{X^{*}}$ (recall that $[x]_{X^{*}}$ denotes the projection of $x$ onto the optimal set $X^{*}$ of convex problem (P)):

They provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. 

Finding a solution of a symmetric linear system can be transformed into optimization problem in the class $q \mathcal{S}_{L_{f}, \kappa_{f}}$.

### RSI

Definition 2 (Restricted secant inequality $-\operatorname{RSI}(\nu))$ a function $f(x): \mathbb{R}^{n} \rightarrow \mathbb{R}$ satisfies the restricted secant inequality (RSI) with constant $\nu>0$ if it is differentiable and obeys
$$
\left\langle\nabla f(x)-\nabla f\left(x_{\mathrm{prj}}\right), x-x_{\mathrm{prj}}\right\rangle \geq \nu\left\|x-x_{\mathrm{prj}}\right\|^{2}
$$
where $x_{\mathrm{prj}}=\operatorname{Proj}_{\mathcal{X} *}(x)$ is the projection of $x$ onto the solution set $\mathcal{X}^{*} .$ Such $f$ is called an $R S I$ function.

### PL

$$\frac{1}{2}\|\nabla f(x)\|^{2} \geq \mu\left(f(x)-f^{*}\right), \quad \forall x$$



还有几个不赘述，关系是$$(S C) \rightarrow(E S C) \rightarrow(W S C) \rightarrow(R S I) \rightarrow(E B) \equiv(P L) \rightarrow(Q G)$$

如果是凸的就有：

$$(R S I) \equiv(E B) \equiv(P L) \equiv(Q G)$$





感觉整体涉及三个方面：

凸/非凸的核心假设

bias的假设

convergence rate

三者互相纠缠，我得回忆一下，这三者之间能取得的各种效果究竟是怎样的。





先回到复杂度上来看：

![image-20210118150822693](C:\Users\sx\AppData\Roaming\Typora\typora-user-images\image-20210118150822693.png)

图取自《优化算法复杂度分析简介》，其中 gravity 代表重心法，ellipsoid 代表椭球法，PGD （Projected Gradient Method ）代表投影梯
度法，AGD ( Accelerated Gradient Method ) 代表加速梯度法，CndG （Conditional Gradient Method ）代表条件梯度法，CGS（Conditional Gradient Sliding Method ）代表加速条件梯度法。表中求解的函 数都是凸函数，且 X ⊆ Rn 是闭且凸的满足 B(r) ⊆ X ⊆ B(R)。其中 Q = L/µ，L 表示梯度的 Lipschitz 常数 µ 表示强凸函数对应的常数。



函数族分类是：

C是连续，右下角是Lipschitz常数L，右上角k次可微和p阶导数（Lipschitz连续），F是强凸，μ是强凸参数，W是弱强凸（弱强凸只在最优点附近满足强凸性质？），S是具有二阶增长性$$f(x)-f^{*} \geq \frac{\mu}{2}\left\|x-x_{*}\right\|^{2}$$函数族之间的关系是 $$\mathcal{F}_{L, \mu}^{1,1}(X) \subset \mathcal{W}_{L, \mu}^{1,1}(X) \subset \mathcal{S}_{L, \mu}^{1,1}(X) \subset \mathcal{F}_{L}^{1,1}(X)$$

貌似这里也是从之前那篇文章里摸来的。





子程序分类是：

ZO (zero order oracle) : 适用于无导数优化算法，对于任意给定的 x0 返回函数值 f(x0). • FO (first order oracle): 适用于梯度法，返回函数在 x0 处的函数值和一阶导数信息，f(x0)，∇f(x0) 或 ∂f(x0).
• 2ndO (second order oracle): 适用于牛顿法，返回函数在 x0 处的函数值和一，二阶导数信息，f(x0)， ∇f(x0) 以及 ∇2f(x0).
1 复杂度分析的理论基础 6
• SFO (stochastic first order oracle): 适用于随机梯度法，返回 x0 在 F(x, ξ) 的函数值和一阶随机梯 度信息 F(x0, ξ0), G(xi, ξ0).
• PO (projection/prox-operator oracle): 适用于投影梯度法，返回 x0 在 X 上的投影 y ∈ argminx∈X ∥x0 − x∥2.
对于求解合成函数 f(x) = g(x) + h(x) 的邻近点梯度法，返回 x0 的邻近点投影 

y ∈ argminx {h(x) + g(x0) + ⟨∇g(x0), x − x0⟩ +L/2∥x0 − x∥2}. 
• LO (linear optimization oracle): 适用于条件梯度法，当 X 是多面体的时，对给定 x0 返回线性规 划的解 y ∈ argminx∈X⟨x0, x⟩.
• SO (separation oracle): 适用于椭球法，对于有界闭约束集合 X ⊂ Rn，给定点 c0 ∈ Rn，如果 c0 ∈ X 则返回真，否则返回一个向量 w 在 c0 处形成一个分割超平面：
wT(x − c0) ≤ 0, ∀x ∈ X.





这里面涉及到的东西太繁杂了，我们举个例子：普通的GD（non-convex下判定条件也有变化，可能得到的是鞍点）：

![img](https://pic2.zhimg.com/80/v2-b697df41c1dc396b08d84479e41979dd_1440w.jpg)

SGD收敛的情况会变成一个一个距离初始点有个常数偏差的线性收敛，那个常数偏差得随着学习率减小而减小（凸的）

Let $f: \mathbf{R}^{d} \rightarrow \mathbf{R}$ has a $\mu$ -Lipschitz gradient, is convex, and is $L$ smooth with respect to $[f, D] .$ Furthermore select $0<\alpha_{i}<\frac{1}{2 L}$ then
$$
\mathbb{E}\left[\left|x_{k}-x^{*}\right|_{2}^{2}\right] \leq(1-\alpha \mu)^{k}\left|x_{0}-x^{*}\right|_{2}^{2}+\frac{2 \alpha \sigma^{2}}{\mu}
$$
进步到非凸的情况：

$$\begin{aligned}
\frac{1}{T} \sum_{t=1}^{T} \mathbb{E}\left[\left\|\nabla F\left(\boldsymbol{x}_{t}\right)\right\|_{2}^{2}\right] & \leq \frac{2 L\left(F\left(\boldsymbol{x}_{1}\right)-F^{\star}\right)}{T}+\left(\frac{2\left(F\left(\boldsymbol{x}_{1}\right)-F^{\star}\right)}{\alpha}+L \alpha\right) \frac{\sigma}{\sqrt{T}}
\end{aligned}$$

速度是$$O(1 / T+\sigma \sqrt{T})$$换算过去也是$$O(1 / \epsilon^{2})$$

接下来到分布式，引入机器数：此时考虑的问题就是在什么delay（就是T，这里会决定系统是ASP还是SSP还是BSP）下能保证一个线性加速（机器数的），如果T限制很松，就代表整个系统可以用ASP，否则还得BSP！

Stochastic gradient is a very powerful approach for large scale optimization. The well known convergence rate of stochastic gradient is O(1/√ K) for convex problems and O(1/K) for strongly
convex problems, Ghadimi and Lan [2013] proved an ergodic convergence rate of O(1/
√ K) for stochastic gradient under nonconvex optimization, which is consistent with the rate of stochastic gradient for convex problems.

synchronous parallel stochastic gradient methods, Dekel et al. [2012] proposed a mini-batch stochastic gradient algorithm – multiple workers compute the stochastic gradient on M data in parallel and need a synchronization step before modifying parameter x in every iteration. The convergence rate is proven to be O(1/√ KM) for convex optimization. 

We review the asynchronous parallel stochastic gradient algorithms in the last. Agarwal and Duchi [2011] analyzed the AsySG-con algorithm (on computer cluster) for convex smooth optimization and proved a convergence rate of O( √1 MK + MT2 K ) which implies that linear speedup is achieved
when T is bounded by O(K1/4/M3/4). In comparison, our analysis for the more general nonconvex smooth optimization improves the upper bound by a factor K1/4M1/4. 

linear speedup is achievable if + nonsmooth convex separable regularization”. Avron et al. [2014] studied this asynchronous T ≤ O(n1/2) for smooth convex functions and T ≤ O(n1/4) for functions with “smooth convex loss parallel stochastic coordinate descent algorithm in solving Ax = b where A is a symmetric positive and T ≤ O(n1/2) for inconsistent read. Tran et al. [2015] studied a semi-asynchronous parallel definite matrix, and showed that the linear speedup is achievable if T ≤ O(n) for consistent read version of Stochastic Dual Coordinate Ascent algorithm which periodically enforces primal-dual synchronization in a separate thread.



到这里我可以再总结一下有哪些地方了：首先是算法，算法有很多，GD，SGD之类的一大堆，每种都有对应的计算复杂度和时间复杂度。

其次是算法的假设：最基础的假设就是strongly-convex，convex还是non-convex还是PL-condition，每种都会对您的problem最后的时间复杂度产生影响。另一种假设是你的noise，比如经典的noise假设就是bounded，也有人说不bounded也可以证明，也有其他的各种bounded的假设。

再其次是分布式的算法：有的是给delay的，有的是做local-sgd的，有的是做sample的，多种多样，这些会带来另一个问题：除了本身的收敛性，时间复杂度之外，还包括能不能linear speed-up，就，你分布式了，能不能分多少加速多少。这个linear speed-up一般会被delay，communication rounds影响，这里就存在一个delay，com的复杂度了。

再其次是一些分布式算法的小改进、动刀，这些基本目标都是加速，但convergence rate基本变不了。speed-up只存在于黑屁中。

我之前一直讨论的是涉及两个方向，一个是这个sgd的convergence问题，另一个是看能不能推到泛化上，后者只能落实到具体的模型，还有一个是我想反查sgd的一些speed-up在某些简单模型上有没有用，因为我们知道很多人在黑屁，但这个黑屁在你们的黑箱模型中表现很好，那窝小走捷径查一下为啥很好。我记得已经有人做了adms之类的在convex上不好，之后得看看这个。

目前的顺序还是先找找asgd的pl，这个我寻思不算个简单的问题。