---
title: 论文阅读记录
categories: [分布式ML]
abbrlink: 21002
date: 2020-1-21 01:30:01
---

# 论文阅读记录

### Asynchronous stochastic convex optimization 

实际情况就是，你发现convex的比nonconvex的难多了，因为convex的P-L condition肯定也要递归之类的，得从convex这边找工具。

$$X_{n} \stackrel{d}{\rightsquigarrow} Z$$

代表随机变量X converges in distribution to 随机变量Z

$$X_{n} \stackrel{L_{p}}{\rightarrow} Z$$

convergence in LP（函数族的限制）

$$X_{n} \stackrel{p}{\rightarrow} Z$$

对任何范数都有个P次的概率收敛

a.s是几乎收敛

$$\mathbb{P}\left(\lim _{n} X_{n} \neq c\right)=0$$

The notation $\mathrm{N}(\mu, \Sigma)$ denotes the multivariate Gaussian with mean $\mu$ and covariance $\Sigma .$ We let $I_{d \times d}$ denote the identity matrix in $\mathbb{R}^{d \times d}$, using $I$ when the dimension is clear from context.

emmmm看来不懂高斯过程，平均场理论是没法活了。



