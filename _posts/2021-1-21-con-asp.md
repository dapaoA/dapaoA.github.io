---
title: 论文阅读记录
categories: [分布式ML]
abbrlink: 21002
date: 2020-1-21 01:30:01
---

# 论文阅读记录

### Asynchronous stochastic convex optimization 

实际情况就是，你发现convex的比nonconvex的难多了，因为convex的P-L condition肯定也要递归之类的，得从convex这边找工具。

$$X_{n} \stackrel{d}{\rightsquigarrow} Z$$

代表随机变量X converges in distribution to 随机变量Z

$$X_{n} \stackrel{L_{p}}{\rightarrow} Z$$

convergence in LP（函数族的限制）

$$X_{n} \stackrel{p}{\rightarrow} Z$$

对任何范数都有个P次的概率收敛

a.s是几乎收敛

$$\mathbb{P}\left(\lim{n}\neq {c}\right)=0$$

The notation $\mathrm{N}(\mu, \Sigma)$ denotes the multivariate Gaussian with mean $\mu$ and covariance $\Sigma .$ We let $I_{d \times d}$ denote the identity matrix in $\mathbb{R}^{d \times d}$, using $I$ when the dimension is clear from context.

emmmm看来不懂高斯过程，平均场理论是没法活了。

Assumption A. There exists $\tau>2$ and a constant $M<\infty$ such that
$$
\sup _{k} \mathbb{E}\left[M_{k}^{\tau}\right]^{\frac{1}{\tau}} \leq M
$$
Assumption B. The function $f$ has unique minimizer $x^{\star}$ and is twice continuously differentiable in the neighborhood of $x^{\star}$ with positive definite Hessian $H=\nabla^{2} f\left(x^{\star}\right) \succ 0 .$ There is a covariance matrix $\Sigma \succ 0$ such that

$$
\mathbb{E}\left[\nabla F\left(x^{\star} ; W\right) \nabla F\left(x^{\star} W\right)^{\top}\right]=\Sigma
$$

Additionally, there exists a constant $C<\infty$ such that the gradients $\nabla F(x ; W)$ satisfy
$$
\mathbb{E}\left[\left\|\nabla F(x ; W)-\nabla F\left(x^{\star} ; W\right)\right\|^{2}\right] \leq C\left\|x-x^{\star}\right\|^{2}, \quad \text { all } x \in \mathbb{R}^{d}
$$
Lastly, $f$ has $L$ -Lipschitz continuous gradient, meaning $\|\nabla f(x)-\nabla f(y)\| \leq L\|x-y\|$ for $x, y \in$ $\mathbb{R}^{d}$

这里就是我们要证明的定理了：

Theorem 1. Let Assumptions $\mathbb{A}$ with moment $\tau>2$ and $\mathbb{B}$ hold. Let the iterates $x_{k}$ be generated by the asynchronous process (ii), (ii), (iii) with stepsize choice $\alpha_{k}=\alpha k^{-\beta}$, where $\beta \in\left(\frac{1}{2}+\frac{1}{\tau-1}, 1\right)$
$$
\frac{1}{\sqrt{n}} \sum_{k=1}^{n}\left(x_{k}-x^{\star}\right) \stackrel{d}{\rightsquigarrow} \mathrm{N}\left(0, H^{-1} \Sigma H^{-1}\right)=\mathrm{N}\left(0,\left(\nabla^{2} f\left(x^{\star}\right)\right)^{-1} \Sigma\left(\nabla^{2} f\left(x^{\star}\right)\right)^{-1}\right)
$$


Specifically, they converge at the optimal rate of 1/n, and we can give explicit constants.
Corollary

Corollary 1. Let the conditions of Theorem 1 hold. Then
$$
n\left(f\left(\bar{x}_{n}\right)-f\left(x^{\star}\right)\right) \stackrel{d}{\rightsquigarrow} \frac{1}{2} \operatorname{tr}\left[H^{-1} \Sigma\right] \cdot \chi_{1}^{2}
$$
where $\chi_{1}^{2}$ denotes a chi-squared random variable with 1 degree of freedom, and $H=\nabla^{2} f\left(x^{\star}\right)$ and $\Sigma=\mathbb{E}\left[\nabla F\left(x^{\star} ; W\right) \nabla F\left(x^{\star} ; W\right)^{\top}\right]$



很不辛的是，我们得先从定理2开始，先证明定理2，再应用定理2，反过来证明定理1

定理2依赖的假设是DEF

Assumption D. There exists a matrix $H \in \mathbb{R}^{d \times d}$ with $H \succ 0,$ a parameter $0<\gamma \leq 1$, constant $C<\infty,$ and some $\epsilon>0$ such that if $x$ satisfies $\left\|x-x^{\star}\right\| \leq \epsilon,$ then
$$
\left\|R(x)-H\left(x-x^{\star}\right)\right\| \leq C\left\|x-x^{\star}\right\|^{1+\gamma} .
$$
不懂，这假设看着好魔幻

Assumption E. The noise vector $\xi(x)$ decomposes as $\xi(x)=\xi(0)+\zeta(x),$ where $\xi(0)$ is a process satisfying $\mathbb{E}\left[\xi_{k}(0) \xi_{k}(0)^{\top} \mid \mathcal{F}_{k-1}\right] \stackrel{p}{\rightarrow} \Sigma \succ 0$ for some matrix $\Sigma \in \mathbb{R}^{d \times d}$, the boundedness condition
$\mathbb{E}\left[\sup _{k} \mathbb{E}\left[\left\|\xi_{k}(0)\right\|^{2} \mid \mathcal{F}_{k-1}\right]\right]<\infty,$ and
$$
\mathbb{E}\left[\left\|\zeta_{k}(x)\right\|^{2} \mid \mathcal{F}_{k-1}\right] \leq C\left\|x-x^{\star}\right\|^{2}
$$
for some constant $C<\infty$ and all $x \in \mathbb{R}^{d}$.

也不懂，对noise vector的某种假设

Assumption $\mathbf{F}$ (Strongly convex residuals). There exists a constant $\lambda_{0}>0$ such that for all 
$$
x \in \mathbb{R}^{d},\left\langle\nabla V\left(x-x^{\star}\right), R(x)\right\rangle \geq \lambda_{0} V\left(x-x^{\star}\right)
$$


Theorem 2. Let V be a function satisfying inequality (7), and let Assumptions D, E, and A hold.

Let the stepsizes $\alpha_{k}=\alpha k^{-\beta},$ where $\frac{1}{\tau-1}+\frac{1}{1+\gamma}<\beta<1 .$ Let one of Assumptions $\mathbb{E}$ or $\mathbb{F}$ hold. Then $x_{n} \stackrel{a . s .}{\rightarrow} x^{\star}$ and
$$
\frac{1}{\sqrt{n}} \sum_{k=1}^{n}\left(x_{k}-x^{\star}\right) \stackrel{d}{\rightsquigarrow} \mathrm{N}\left(0, H^{-1} \Sigma H^{-1}\right)
$$
inequality (7)是

We assume there is a Lyapunov function $V$ that functions (essentially) as a squared norm, which satisfies $V(x) \geq \lambda\|x\|^{2}$ for all $x \in \mathbb{R}^{d}$ $\|\nabla V(x)-\nabla V(y)\| \leq L\|x-y\|$ for all $x, y,$ that $\nabla V(0)=0,$ and $V(0)=0 .$ Note in particular that this implies
$$
\lambda\|x\|^{2} \leq V(x) \leq V(0)+\langle\nabla V(0), x-0\rangle+\frac{L}{2}\|x\|^{2}=\frac{L}{2}\|x\|^{2}
$$
and that $\|\nabla V(x)\|^{2} \leq L^{2}\|x\|^{2} \leq\left(L^{2} / \lambda\right) V(x)$. In addition, we make the following assumptions on the residual function (cf. [26, Assumption 3.2]).

整体思路：

1：delay在异步更新时有一个上界$$n^{\rho}$$,（就是假设A？，证明是lemma3，lemma4）

2：证明正确序列$$\widetilde{x}_{k}$$恰当收敛，也就是正确序列下的“正确”误差不发散（不清楚，可能正确序列是不用delay的那种，appropriately收敛也不清楚，lemma5）

3：利用Robbins-Siegmund 边际收敛定理证明实际误差不发散（lemma2）

4：证明实际误差，正确误差，和另一个矩阵构造出来的误差是概率上渐进等价的。（lemma 6，7，8）

5：证明两个误差的范数增长的比例$$\left\|\Delta_{k}-\widetilde{\Delta}_{k}\right\|^{2}=O_{P}\left(\alpha_{k}^{2} k^{2 \rho}\right)$$ 是步长的二次。

6：Asymptotic normality of $$\Delta_{k}, \widetilde{\Delta}_{k}, \widetilde{\Delta}_{k}^{\prime}$$ (Lemma10）保证了中间这个构造序列的中心极限定理

7：证明实际误差不发散（lemma 11）





