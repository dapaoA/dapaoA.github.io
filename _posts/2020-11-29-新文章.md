---
title: 论文阅读记录
categories: [分布式ML]
abbrlink: 99001
date: 2020-1-6 01:30:01
---

# 论文阅读记录

### 2020-1-5

> Heterogeneity-aware Distributed Parameter Servers  

(佛了，typora没有关闭时询问保存提醒，我是傻逼)

看懂了ConSDG和DsySDG的原理，前者x1/M简单有效到让人怀疑作者是不是把业界的约定俗成拿来凑了一篇文章，后者看懂之后反而觉得怪怪的，可能是参数服务器也不是一台有一个集群需要互相交换数据的原因才使我现在充满疑惑，这部分明天看。

我现在不清楚DsySDG找到传给不同worker同版本w参数的数目并以此做参数这种归一化手段1：是否能有效抑制慢worker(或者有没有必要抑制慢worker)，2：真的会有同一个w样本被几个worker一起用吗？只要一个worker传上参数，做个加法w就更新了，这一个加法的时间内能有几个一起来正好都上传完了取一份？3：为什么还要在参数上传的时候去改变staleness:S的值？不应该发放的时候就知道发了几份吗？

### 2019-12-16之前

> MG-WFBP: Efficient Data Communication for
> Distributed Synchronous SGD Algorithms

老师给出的三篇文章的第一篇，重要的部分是边传边算，比较重要的部分是如果没传完已经算出来了可以一起传更快（一个拟线性）。

> A Distributed Synchronous SGD Algorithm with
> Global Top-k Sparsification for Low Bandwidth
> Networks  

提出了gtop-k，一个树结构的传输把复杂度降到logP（P是worker个数），最后汇总到根上由根算出top-k再broadcast到所有节点上

> A Convergence Analysis of Distributed SGD with Communication-Efficient
> Gradient Sparsification  

证明了上面那个的算法收敛性